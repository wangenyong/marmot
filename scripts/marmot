#!/bin/bash

# get script current dir and project home dir
SCRIPT_DIR=$(dirname -- "$(readlink -f -- "$BASH_SOURCE")")
HOME_DIR="$(dirname $SCRIPT_DIR)"

# print logo
cat $HOME_DIR/conf/welcome.figlet
printf -- '\n'
printf -- '\n'

# loading config file
source $HOME_DIR/conf/config.conf
# loading printf file
source $HOME_DIR/conf/printf.conf
# loading environment
source /etc/profile
# loading cluster nodes
IFS=',' read -ra workers <<<$HADOOP_WORKERS
IFS=',' read -ra azkaban_nodes <<<$AZKABAN_NODES
IFS=',' read -ra zookeeper_nodes <<<$ZOOKEEPER_NODES
IFS=',' read -ra kafka_nodes <<<$KAFKA_NODES
IFS=',' read -ra kettle_nodes <<<$KETTLE_NODES
IFS=',' read -ra dolphinscheduler_nodes <<<$DOLPHINSCHEDULER_NODES

# no arguments
if [ ${#@} -eq 0 ]; then
    if [ -d "$PROJECT_DIR" ]; then
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            printf -- "${SUCCESS}The hadoop has been installed.${END}\n"
            printf -- "You can use it by command: ${BOLD}${NORMAL}marmot { start | status | stop } hadoop${END}\n"
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed.${END}\n"
            printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
    else
        printf -- "${WARN}The project haven't been initialized.${END}\n"
        printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install { hadoop | ds | kafka }${END}\n"
        printf -- '\n'
    fi
    exit 0
fi

function check_process() {
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print$2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

case "$1" in
install)
    case "$2" in
    "hadoop")
        sh $SCRIPT_DIR/config-environment.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-jdk.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hadoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-mysql.sh -F
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hive.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-spark.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-flink.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-sqoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-datax.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-zookeeper.sh
        printf -- '\n'
        printf -- '\n'
        ;;
    "ds")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-dolphinscheduler.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "hbase")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-hbase.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "kafka")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-kafka.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot install { hadoop | ds | hbase | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
start)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== START HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Start hdfs.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/sbin/start-dfs.sh"
        printf -- "${INFO}>>> Start yarn.${END}\n"
        ssh $HADOOP_USER@$YARN_RM "$HADOOP_HOME/sbin/start-yarn.sh"
        printf -- "${INFO}>>> Start hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/bin/mapred --daemon start historyserver"
        printf -- "${INFO}>>> Start hive.${END}\n"
        printf -- "${INFO}--> Start HiveMetastore.${END}\n"
        metapid=$(check_process HiveMetastore 9083)
        cmd="ssh $HADOOP_USER@$MYSQL_HOST nohup hive --service metastore >$HIVE_HOME/logs/metastore.log 2>&1 &"
        [ -z "$metapid" ] && eval $cmd || printf -- "${WARN}Metastroe started.${END}\n"
        printf -- "${INFO}--> Start HiveServer2.${END}\n"
        server2pid=$(check_process HiveServer2 10000)
        cmd="ssh $HADOOP_USER@$MYSQL_HOST nohup hiveserver2 >$HIVE_HOME/logs/hiveServer2.log 2>&1 &"
        [ -z "$server2pid" ] && eval $cmd || printf -- "${WARN}Hiveserver2 started.${END}\n"
        printf -- "${INFO}>>> Start spark historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$SPARK_HOME/sbin/start-history-server.sh"
        ;;
    "zk")
        printf -- "${INFO}========== START ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Start $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh start"
        done
        ;;
    "ds")
        printf -- "${INFO}========== START DOLPHINSCHEDULER ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/start-all.sh"
        ;;
    "hbase")
        printf -- "${INFO}========== START HBASE ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HBASE_HOME/bin/start-hbase.sh"
        ;;
    "kafka")
        printf -- "${INFO}========== START KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Start $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"
        done
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot start { hadoop | zk | ds | hbase | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
stop)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== STOP HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Stop spark historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$SPARK_HOME/sbin/stop-history-server.sh"
        printf -- "${INFO}>>> Stop hive.${END}\n"
        printf -- "${INFO}--> Stop HiveMetastore.${END}\n"
        metapid=$(check_process HiveMetastore 9083)
        [ "$metapid" ] && kill $metapid || printf -- "${WARN}Metastroe have not started.${END}\n"
        printf -- "${INFO}--> Stop HiveServer2.${END}\n"
        server2pid=$(check_process HiveServer2 10000)
        [ "$server2pid" ] && kill $server2pid || printf -- "${WARN}HiveServer2 have not started.${END}\n"
        printf -- "${INFO}>>> Stop hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/bin/mapred --daemon stop historyserver"
        printf -- "${INFO}>>> Stop yarn.${END}\n"
        ssh $HADOOP_USER@$YARN_RM "$HADOOP_HOME/sbin/stop-yarn.sh"
        printf -- "${INFO}>>> Stop hdfs.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/sbin/stop-dfs.sh"
        ;;
    "zk")
        printf -- "${INFO}========== STOP ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Stop $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh stop"
        done
        ;;
    "ds")
        printf -- "${INFO}========== STOP DOLPHINSCHEDULER ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/stop-all.sh"
        ;;
    "hbase")
        printf -- "${INFO}========== STOP HBASE ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HBASE_HOME/bin/stop-hbase.sh"
        ;;
    "kafka")
        printf -- "${INFO}========== STOP KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Stop $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-stop.sh"
        done
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot stop { hadoop | zk | ds | hbase | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
status)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== HADOOP CLUSTERS STATUS ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host jps
        done
        printf -- "${INFO}----- hive services status -----${END}\n"
        check_process HiveMetastore 9083 >/dev/null && printf -- "${SUCCESS}HiveMetastore running normally.${END}\n" || printf -- "${WARN}HiveMetastore running abnormally.${END}\n"
        check_process HiveServer2 10000 >/dev/null && printf -- "${SUCCESS}HiveServer2 running normally${END}\n" || printf -- "${WARN}HiveServer2 running abnormally.${END}\n"
        ;;
    "zk")
        printf -- "${INFO}========== ZOOKEEPER STATUS ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh status"
        done
        ;;
    "ds")
        printf -- "${INFO}========== DOLPHINSCHEDULER STATUS ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/status-all.sh"
        ;;
    "kafka")
        printf -- 'kafka status\n'
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot status { hadoop | zk | ds | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
rm)
    case "$2" in
    "hadoop")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            printf -- "${WARN}Do you want to remove hadoop.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host hadoop -----${END}\n"
                    ssh $host "rm -rf /opt/marmot/*; cat /dev/null >/etc/profile.d/marmot_env.sh"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The hadoop haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "ds")
        if [ -d $PROJECT_DIR/dolphinscheduler ]; then
            printf -- "${WARN}Do you want to remove dolphinscheduler.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${dolphinscheduler_nodes[@]}; do
                    printf -- "${INFO}----- remove $host dolphinscheduler -----${END}\n"
                    ssh $host "rm -rf $PROJECT_DIR/dolphinscheduler"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "hbase")
        if [ $HBASE_HOME ] && [ -d $HBASE_HOME ]; then
            printf -- "${WARN}Do you want to remove hbase.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host hbase -----${END}\n"
                    ssh $host "rm -rf $HBASE_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The hbase haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "kafka")
        if [ $KAFKA_HOME ] && [ -d $KAFKA_HOME ]; then
            printf -- "${WARN}Do you want to remove kafka.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${kafka_nodes[@]}; do
                    printf -- "${INFO}----- remove $host kafka -----${END}\n"
                    ssh $host "rm -rf $KAFKA_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot rm { hadoop | ds | hbase | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
*)
    echo "default (none of above)"
    ;;
esac
