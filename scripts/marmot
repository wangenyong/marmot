#!/bin/bash

# get script current dir and project home dir
SCRIPT_DIR=$(dirname -- "$(readlink -f -- "$BASH_SOURCE")")
HOME_DIR="$(dirname $SCRIPT_DIR)"

# print logo
cat $HOME_DIR/conf/welcome.figlet
printf -- '\n'
printf -- '\n'

# loading config file
source $HOME_DIR/conf/config.conf
# loading printf file
source $HOME_DIR/conf/printf.conf
# loading version file
source $HOME_DIR/conf/version.conf
# loading environment
source /etc/profile
# loading cluster nodes
IFS=',' read -ra workers <<<$HADOOP_WORKERS
IFS=',' read -ra azkaban_nodes <<<$AZKABAN_NODES
IFS=',' read -ra zookeeper_nodes <<<$ZOOKEEPER_NODES
IFS=',' read -ra kafka_nodes <<<$KAFKA_NODES
IFS=',' read -ra kettle_nodes <<<$KETTLE_NODES
IFS=',' read -ra dolphinscheduler_nodes <<<$DOLPHINSCHEDULER_NODES
IFS=',' read -ra solr_nodes <<<$SOLR_NODES
IFS=',' read -ra prometheus_nodes <<<$PROMETHEUS_NODES

# no arguments
if [ ${#@} -eq 0 ]; then
    if [ -d "$PROJECT_DIR" ]; then
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            printf -- "${SUCCESS}The hadoop has been installed.${END}\n"
            printf -- "You can use it by command: ${BOLD}${NORMAL}marmot { start | status | stop } hadoop${END}\n"
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed.${END}\n"
            printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
    else
        printf -- "${WARN}The project haven't been initialized.${END}\n"
        printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install { hadoop | ds | kafka }${END}\n"
        printf -- '\n'
    fi
    exit 0
fi

function check_process() {
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print$2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

function start_hive() {
    printf -- "${INFO}--> Start HiveMetastore.${END}\n"
    metapid=$(check_process HiveMetastore 9083)
    cmd="ssh $HADOOP_USER@$HIVE_SERVER nohup hive --service metastore >$HIVE_HOME/logs/metastore.log 2>&1 &"
    [ -z "$metapid" ] && eval $cmd || printf -- "${WARN}Metastroe started.${END}\n"

    printf -- "${INFO}--> Start HiveServer2.${END}\n"
    server2pid=$(check_process HiveServer2 10000)
    cmd="ssh $HADOOP_USER@$HIVE_SERVER nohup hiveserver2 >$HIVE_HOME/logs/hiveServer2.log 2>&1 &"
    [ -z "$server2pid" ] && eval $cmd || printf -- "${WARN}Hiveserver2 started.${END}\n"
}

function stop_hive() {
    printf -- "${INFO}--> Stop HiveMetastore.${END}\n"
    metapid=$(check_process HiveMetastore 9083)
    [ "$metapid" ] && kill $metapid || printf -- "${WARN}Metastroe have not started.${END}\n"

    printf -- "${INFO}--> Stop HiveServer2.${END}\n"
    server2pid=$(check_process HiveServer2 10000)
    [ "$server2pid" ] && kill $server2pid || printf -- "${WARN}HiveServer2 have not started.${END}\n"
}

function start_prometheus() {
    printf -- "${INFO}--> Start prometheus server.${END}\n"
    prometheus_pid=$(check_process prometheus 9090)
    cmd="ssh $HADOOP_USER@$PROMETHEUS_SERVER \"cd $PROJECT_DIR/prometheus-${prometheus_version}; nohup ./prometheus --config.file=prometheus.yml > ./prometheus.log 2>&1 &\""
    [ -z "$prometheus_pid" ] && eval $cmd || printf -- "${WARN}Prometheus server started.${END}\n"

    printf -- "${INFO}--> Start prometheus pushgateway.${END}\n"
    pushgateway_pid=$(check_process pushgateway 9091)
    cmd="ssh $HADOOP_USER@$PROMETHEUS_SERVER \"cd $PROJECT_DIR/pushgateway-${pushgateway_version}; nohup ./pushgateway --web.listen-address :9091 > ./pushgateway.log 2>&1 &\""
    [ -z "$pushgateway_pid" ] && eval $cmd || printf -- "${WARN}Prometheus pushgateway started.${END}\n"

    printf -- "${INFO}--> Start grafana.${END}\n"
    grafana_pid=$(check_process grafana-server 3000)
    cmd="ssh $HADOOP_USER@$PROMETHEUS_SERVER \"cd $PROJECT_DIR/grafana-${grafana_version}; nohup ./bin/grafana-server web > ./grafana.log 2>&1 &\""
    [ -z "$grafana_pid" ] && eval $cmd || printf -- "${WARN}Grafana started.${END}\n"
    
}

function stop_prometheus() {
    printf -- "${INFO}--> Stop prometheus server.${END}\n"
    prometheus_pid=$(check_process prometheus 9090)
    [ "$prometheus_pid" ] && kill $prometheus_pid || printf -- "${WARN}Prometheus server have not started.${END}\n"

    printf -- "${INFO}--> Stop prometheus pushgateway.${END}\n"
    pushgateway_pid=$(check_process pushgateway 9091)
    [ "$pushgateway_pid" ] && kill $pushgateway_pid || printf -- "${WARN}Prometheus pushgateway have not started.${END}\n"

    printf -- "${INFO}--> Stop grafana.${END}\n"
    grafana_pid=$(check_process grafana-server 3000)
    [ "$grafana_pid" ] && kill $grafana_pid || printf -- "${WARN}Grafana have not started.${END}\n"
}

case "$1" in
install)
    case "$2" in
    "hadoop")
        sh $SCRIPT_DIR/config-environment.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-jdk.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hadoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-mysql.sh -F
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hive.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-spark.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-flink.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-sqoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-datax.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-zookeeper.sh
        printf -- '\n'
        printf -- '\n'
        ;;
    "ds")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-dolphinscheduler.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "hbase")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-hbase.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "kafka")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-kafka.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "solr")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-solr.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "atlas")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-atlas.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "pm")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-prometheus.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "${WARN}The hadoop haven't been installed. You should install it first.${END}\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot install { hadoop | ds | hbase | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
start)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== START HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Start hdfs.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/sbin/start-dfs.sh"
        printf -- "${INFO}>>> Start yarn.${END}\n"
        ssh $HADOOP_USER@$YARN_RM "$HADOOP_HOME/sbin/start-yarn.sh"
        printf -- "${INFO}>>> Start hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/bin/mapred --daemon start historyserver"
        printf -- "${INFO}>>> Start hive.${END}\n"
        start_hive
        printf -- "${INFO}>>> Start spark historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$SPARK_HOME/sbin/start-history-server.sh"
        ;;
    "hive")
        printf -- "${INFO}========== START HIVE ==========${END}\n"
        start_hive
        ;; 
    "zk")
        printf -- "${INFO}========== START ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Start $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh start"
        done
        ;;
    "ds")
        printf -- "${INFO}========== START DOLPHINSCHEDULER ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/start-all.sh"
        ;;
    "hbase")
        printf -- "${INFO}========== START HBASE ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HBASE_HOME/bin/start-hbase.sh"
        ;;
    "kafka")
        printf -- "${INFO}========== START KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Start $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"
        done
        ;;
    "solr")
        printf -- "${INFO}========== START SOLR ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}--> Start $host solr.${END}\n"
            ssh $SOLR_USER@$host "$PROJECT_DIR/solr-${solr_version}/bin/solr start"
        done
        ;;
    "atlas")
        printf -- "${INFO}========== START ATLAS ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$PROJECT_DIR/atlas-${atlas_version}/bin/atlas_start.py"
        ;;
    "pm")
        printf -- "${INFO}========== START PROMETHEUS ==========${END}\n"
        start_prometheus
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot start { hadoop | zk | ds | hbase | kafka | solr }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
stop)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== STOP HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Stop spark historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$SPARK_HOME/sbin/stop-history-server.sh"
        printf -- "${INFO}>>> Stop hive.${END}\n"
        stop_hive
        printf -- "${INFO}>>> Stop hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/bin/mapred --daemon stop historyserver"
        printf -- "${INFO}>>> Stop yarn.${END}\n"
        ssh $HADOOP_USER@$YARN_RM "$HADOOP_HOME/sbin/stop-yarn.sh"
        printf -- "${INFO}>>> Stop hdfs.${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HADOOP_HOME/sbin/stop-dfs.sh"
        ;;
    "hive")
        printf -- "${INFO}========== STOP HIVE ==========${END}\n"
        stop_hive
        ;;    
    "zk")
        printf -- "${INFO}========== STOP ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Stop $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh stop"
        done
        ;;
    "ds")
        printf -- "${INFO}========== STOP DOLPHINSCHEDULER ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/stop-all.sh"
        ;;
    "hbase")
        printf -- "${INFO}========== STOP HBASE ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$HBASE_HOME/bin/stop-hbase.sh"
        ;;
    "kafka")
        printf -- "${INFO}========== STOP KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Stop $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-stop.sh"
        done
        ;;
    "solr")
        printf -- "${INFO}========== STOP SOLR ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}--> Stop $host solr.${END}\n"
            ssh $SOLR_USER@$host "$PROJECT_DIR/solr-${solr_version}/bin/solr stop"
        done
        ;;
    "atlas")
        printf -- "${INFO}========== STOP ATLAS ==========${END}\n"
        ssh $HADOOP_USER@$HDFS_NAMENODE "$PROJECT_DIR/atlas-${atlas_version}/bin/atlas_stop.py"
        ;;
    "pm")
        printf -- "${INFO}========== STOP PROMETHEUS ==========${END}\n"
        stop_prometheus
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot stop { hadoop | zk | ds | hbase | kafka | solr }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
status)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== HADOOP CLUSTERS STATUS ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host jps
        done
        printf -- "${INFO}----- hive services status -----${END}\n"
        check_process HiveMetastore 9083 >/dev/null && printf -- "${SUCCESS}HiveMetastore running normally.${END}\n" || printf -- "${WARN}HiveMetastore running abnormally.${END}\n"
        check_process HiveServer2 10000 >/dev/null && printf -- "${SUCCESS}HiveServer2 running normally.${END}\n" || printf -- "${WARN}HiveServer2 running abnormally.${END}\n"
        ;;
    "zk")
        printf -- "${INFO}========== ZOOKEEPER STATUS ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh status"
        done
        ;;
    "ds")
        printf -- "${INFO}========== DOLPHINSCHEDULER STATUS ==========${END}\n"
        ssh $HADOOP_USER@$DOLPHINSCHEDULER_MASTER "$DOLPHINSCHEDULER_HOME/bin/status-all.sh"
        ;;
    "kafka")
        printf -- 'kafka status\n'
        ;;
    "solr")
        printf -- "${INFO}========== SOLR STATUS ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $SOLR_USER@$host "$PROJECT_DIR/solr-${solr_version}/bin/solr status"
        done
        ;;
    "pm")
        printf -- "${INFO}========== PROMETHEUS STATUS ==========${END}\n"
        check_process prometheus 9090 >/dev/null && printf -- "${SUCCESS}Prometheus server running normally.${END}\n" || printf -- "${WARN}Prometheus server running abnormally.${END}\n"
        check_process pushgateway 9091 >/dev/null && printf -- "${SUCCESS}Prometheus pushgateway running normally.${END}\n" || printf -- "${WARN}Prometheus pushgateway running abnormally.${END}\n"
        check_process grafana-server 3000 >/dev/null && printf -- "${SUCCESS}Grafana running normally.${END}\n" || printf -- "${WARN}Grafana running abnormally.${END}\n"
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot status { hadoop | zk | ds | kafka | solr }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
rm)
    case "$2" in
    "hadoop")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            printf -- "${WARN}Do you want to remove hadoop.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host hadoop -----${END}\n"
                    ssh $host "rm -rf /opt/marmot/*; cat /dev/null >/etc/profile.d/marmot_env.sh"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The hadoop haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "zk")
        if [ $ZOOKEEPER_HOME ] && [ -d $ZOOKEEPER_HOME ]; then
            printf -- "${WARN}Do you want to remove zookeeper.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${zookeeper_nodes[@]}; do
                    printf -- "${INFO}----- remove $host zookeeper -----${END}\n"
                    ssh $host "rm -rf $ZOOKEEPER_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "ds")
        if [ -d $PROJECT_DIR/dolphinscheduler ]; then
            printf -- "${WARN}Do you want to remove dolphinscheduler.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${dolphinscheduler_nodes[@]}; do
                    printf -- "${INFO}----- remove $host dolphinscheduler -----${END}\n"
                    ssh $host "rm -rf $DOLPHINSCHEDULER_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "hbase")
        if [ $HBASE_HOME ] && [ -d $HBASE_HOME ]; then
            printf -- "${WARN}Do you want to remove hbase.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host hbase -----${END}\n"
                    ssh $host "rm -rf $HBASE_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The hbase haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "kafka")
        if [ $KAFKA_HOME ] && [ -d $KAFKA_HOME ]; then
            printf -- "${WARN}Do you want to remove kafka.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${kafka_nodes[@]}; do
                    printf -- "${INFO}----- remove $host kafka -----${END}\n"
                    ssh $host "rm -rf $KAFKA_HOME"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "solr")
        if [ -d "$PROJECT_DIR/solr-${solr_version}" ]; then
            printf -- "${WARN}Do you want to remove solr.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${solr_nodes[@]}; do
                    printf -- "${INFO}----- remove $host solr -----${END}\n"
                    ssh $host "rm -rf $PROJECT_DIR/solr-${solr_version}"
                done
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The solr haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "atlas")
        if [ -d "$PROJECT_DIR/atlas-${atlas_version}" ]; then
            printf -- "${WARN}Do you want to remove atlas.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                rm -rf $PROJECT_DIR/atlas-${atlas_version}
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The solr haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    "pm")
        if [ -d "$PROJECT_DIR/prometheus-${prometheus_version}" ]; then
            printf -- "${WARN}Do you want to remove prometheus.${END}\n"
            printf -- " ${ERROR}[y|Y]${END} or ${ERROR}[n|N]${END} : "
            read choice
            case $choice in
            [yY]*)
                for host in ${prometheus_nodes[@]}; do
                    printf -- "${INFO}----- remove $host prometheus node exporter -----${END}\n"
                    ssh $host "systemctl stop node_exporter.service"
                    ssh $host "rm -f /usr/lib/systemd/system/node_exporter.service"
                    ssh $host "rm -rf $PROJECT_DIR/node_exporter-${node_exporter_version}"
                done
                printf -- "${INFO}----- remove prometheus pushgateway -----${END}\n"
                rm -rf $PROJECT_DIR/pushgateway-${pushgateway_version}
                printf -- "${INFO}----- remove prometheus server -----${END}\n"
                rm -rf $PROJECT_DIR/prometheus-${prometheus_version}
                printf -- "${INFO}----- remove grafana -----${END}\n"
                rm -rf $PROJECT_DIR/grafana-${grafana_version}
                ;;
            [nN]*) printf -- "${WARN}Remove cancelled.${END}\n" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The prometheus haven't been installed.${END}\n"
            printf -- '\n'
        fi
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot rm { hadoop | ds | hbase | kafka | solr }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
*)
    echo "default (none of above)"
    ;;
esac
