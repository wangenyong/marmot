#!/bin/bash

# get script current dir and project home dir
SCRIPT_DIR=$(dirname -- "$(readlink -f -- "$BASH_SOURCE")")
HOME_DIR="$(dirname $SCRIPT_DIR)"

# print logo
cat $HOME_DIR/conf/welcome.figlet
printf -- '\n'
printf -- '\n'

# loading config file
source $HOME_DIR/conf/config.conf
# loading printf file
source $HOME_DIR/conf/printf.conf
# loading cluster nodes
IFS=',' read -ra workers <<<$HADOOP_WORKERS

# no arguments
if [ ${#@} -eq 0 ]; then
    if [ ! -d "$PROJECT_DIR" ]; then
        printf -- "The project have not been installed.\n"
        printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install { hadoop | kettle | kafka }${END}\n"
        printf -- '\n'
    fi
    exit 0
fi

case "$1" in
install)
    case "$2" in
    "hadoop")
        sh $SCRIPT_DIR/config-environment.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-jdk.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hadoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-mysql.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hive.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-spark.sh
        printf -- '\n'
        printf -- '\n'
        ;;
    "kettle")
        printf -- "kettle\n"
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot install { hadoop | kettle | kafka }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
start)
    printf -- "${INFO}========== START HADOOP CLUSTERS ==========${END}\n"
    printf -- "${INFO}>>> Start hdfs.${END}\n"
    ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/sbin/start-dfs.sh"
    printf -- "${INFO}>>> Start yarn.${END}\n"
    ssh $HADOOP_USER@${workers[1]} "$HADOOP_HOME/sbin/start-yarn.sh"
    printf -- "${INFO}>>> Start hadoop historyserver.${END}\n"
    ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/bin/mapred --daemon start historyserver"
    ;;
stop)
    printf -- "${INFO}========== CLOSE HADOOP CLUSTERS ==========${END}\n"
    printf -- "${INFO}>>> Stop hadoop historyserver.${END}\n"
    ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/bin/mapred --daemon stop historyserver"
    printf -- "${INFO}>>> Stop yarn.${END}\n"
    ssh $HADOOP_USER@${workers[1]} "$HADOOP_HOME/sbin/stop-yarn.sh"
    printf -- "${INFO}>>> Stop hdfs.${END}\n"
    ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/sbin/stop-dfs.sh"
    ;;
status)
    for host in ${workers[@]}; do
        printf -- "${INFO}----- $host -----${END}\n"
        ssh $host jps
    done
    ;;
remove)
    for host in ${workers[@]}; do
        printf -- "${INFO}----- $host -----${END}\n"
        ssh $host "rm -rf /opt/marmot/*; cat /dev/null >/etc/profile.d/marmot_env.sh"
    done
    ;;
*)
    echo "default (none of above)"
    ;;
esac
