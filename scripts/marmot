#!/bin/bash

# get script current dir and project home dir
SCRIPT_DIR=$(dirname -- "$(readlink -f -- "$BASH_SOURCE")")
HOME_DIR="$(dirname $SCRIPT_DIR)"

# print logo
cat $HOME_DIR/conf/welcome.figlet
printf -- '\n'
printf -- '\n'

# loading config file
source $HOME_DIR/conf/config.conf
# loading printf file
source $HOME_DIR/conf/printf.conf
# loading environment
source /etc/profile
# loading cluster nodes
IFS=',' read -ra workers <<<$HADOOP_WORKERS
IFS=',' read -ra azkaban_nodes <<<$AZKABAN_NODES
IFS=',' read -ra zookeeper_nodes <<<$ZOOKEEPER_NODES
IFS=',' read -ra kafka_nodes <<<$KAFKA_NODES
IFS=',' read -ra kettle_nodes <<<$KETTLE_NODES

# no arguments
if [ ${#@} -eq 0 ]; then
    if [ -d "$PROJECT_DIR" ]; then
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            printf -- "The hadoop has been installed.\n"
            printf -- "You can use it by command: ${BOLD}${NORMAL}marmot { start | status | stop } hadoop${END}\n"
            printf -- '\n'
        else
            printf -- "The hadoop haven't been installed.\n"
            printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
    else
        printf -- "The project haven't been initialized.\n"
        printf -- "You can install it by command: ${BOLD}${NORMAL}marmot install { hadoop | azkaban | kafka | kettle }${END}\n"
        printf -- '\n'
    fi
    exit 0
fi

function check_process() {
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print$2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

case "$1" in
install)
    case "$2" in
    "hadoop")
        sh $SCRIPT_DIR/config-environment.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-jdk.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hadoop.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-mysql.sh -F
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-hive.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-spark.sh
        printf -- '\n'
        printf -- '\n'
        ;;
    "azkaban")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-azkaban.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "The hadoop haven't been installed. You should install it first.\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "kafka")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            sh $SCRIPT_DIR/deploy-zookeeper.sh
            sh $SCRIPT_DIR/deploy-kafka.sh
            printf -- '\n'
            printf -- '\n'
        else
            printf -- "The hadoop haven't been installed. You should install it first.\n"
            printf -- "You can install hadoop by command: ${BOLD}${NORMAL}marmot install hadoop${END}\n"
            printf -- '\n'
        fi
        ;;
    "kettle")
        sh $SCRIPT_DIR/config-environment.sh $2
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-jdk.sh $2
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-mysql.sh
        printf -- '\n'
        printf -- '\n'
        sh $SCRIPT_DIR/deploy-kettle.sh
        printf -- '\n'
        printf -- '\n'
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot install { hadoop | azkaban | kafka | kettle }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
start)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== START HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Start hdfs.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/sbin/start-dfs.sh"
        printf -- "${INFO}>>> Start yarn.${END}\n"
        ssh $HADOOP_USER@${workers[1]} "$HADOOP_HOME/sbin/start-yarn.sh"
        printf -- "${INFO}>>> Start hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/bin/mapred --daemon start historyserver"
        printf -- "${INFO}>>> Start hive.${END}\n"
        printf -- "${INFO}--> Start HiveMetastore.${END}\n"
        metapid=$(check_process HiveMetastore 9083)
        cmd="ssh $HADOOP_USER@${workers[0]} nohup hive --service metastore >$HIVE_HOME/logs/metastore.log 2>&1 &"
        [ -z "$metapid" ] && eval $cmd || printf -- "${WARN}Metastroe started.${END}\n"
        printf -- "${INFO}--> Start HiveServer2.${END}\n"
        server2pid=$(check_process HiveServer2 10000)
        cmd="ssh $HADOOP_USER@${workers[0]} nohup hiveserver2 >$HIVE_HOME/logs/hiveServer2.log 2>&1 &"
        [ -z "$server2pid" ] && eval $cmd || printf -- "${WARN}Hiveserver2 started.${END}\n"
        printf -- "${INFO}>>> Start spark historyserver.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$SPARK_HOME/sbin/start-history-server.sh"
        ;;
    "azkaban")
        printf -- "${INFO}========== START AZKABAN ==========${END}\n"
        printf -- "${INFO}>>> Start azkaban executor.${END}\n"
        for host in ${azkaban_nodes[@]}; do
            printf -- "${INFO}--> Start $host executor.${END}\n"
            ssh $HADOOP_USER@$host "cd $AZKABAN_HOME/azkaban-exec; bin/start-exec.sh"
            sleep 5s
            ssh $HADOOP_USER@$host "cd $AZKABAN_HOME/azkaban-exec; curl -G \"$host:\$(<./executor.port)/executor?action=activate\" && echo "
        done
        printf -- "\n"
        printf -- "${INFO}>>> Start azkaban web server.${END}\n"
        ssh $HADOOP_USER@${azkaban_nodes[0]} "cd $AZKABAN_HOME/azkaban-web; bin/start-web.sh"
        ;;
    "zookeeper")
        printf -- "${INFO}========== START ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Start $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh start"
        done
        ;;
    "kafka")
        printf -- "${INFO}========== START KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Start $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties"
        done
        ;;
    "kettle")
        printf -- "${INFO}========== START KETTLE CLUSTERS ==========${END}\n"
        KETTLE_HOME=/opt/marmot/data-integration
        i=0
        for host in ${kettle_nodes[@]}; do
            printf -- "${INFO}--> Start $host kettle.${END}\n"
            ssh $KETTLE_USER@$host "nohup $KETTLE_HOME/carte.sh $host 808$i >$KETTLE_HOME/logs/kettle.log 2>&1 &"
            let i+=1
        done
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot start { hadoop | azkaban | zookeeper | kafka | kettle }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
stop)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== STOP HADOOP CLUSTERS ==========${END}\n"
        printf -- "${INFO}>>> Stop spark historyserver.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$SPARK_HOME/sbin/stop-history-server.sh"
        printf -- "${INFO}>>> Stop hive.${END}\n"
        printf -- "${INFO}--> Stop HiveMetastore.${END}\n"
        metapid=$(check_process HiveMetastore 9083)
        [ "$metapid" ] && kill $metapid || printf -- "${WARN}Metastroe have not started.${END}\n"
        printf -- "${INFO}--> Stop HiveServer2.${END}\n"
        server2pid=$(check_process HiveServer2 10000)
        [ "$server2pid" ] && kill $server2pid || printf -- "${WARN}HiveServer2 have not started.${END}\n"
        printf -- "${INFO}>>> Stop hadoop historyserver.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/bin/mapred --daemon stop historyserver"
        printf -- "${INFO}>>> Stop yarn.${END}\n"
        ssh $HADOOP_USER@${workers[1]} "$HADOOP_HOME/sbin/stop-yarn.sh"
        printf -- "${INFO}>>> Stop hdfs.${END}\n"
        ssh $HADOOP_USER@${workers[0]} "$HADOOP_HOME/sbin/stop-dfs.sh"
        ;;
    "azkaban")
        printf -- "${INFO}========== STOP AZKABAN ==========${END}\n"
        printf -- "${INFO}>>> Stop azkaban web server.${END}\n"
        ssh $HADOOP_USER@${azkaban_nodes[0]} "cd $AZKABAN_HOME/azkaban-web; bin/shutdown-web.sh"
        printf -- "\n"
        for host in ${azkaban_nodes[@]}; do
            printf -- "${INFO}--> Stop $host executor.${END}\n"
            ssh $HADOOP_USER@$host "cd $AZKABAN_HOME/azkaban-exec; bin/shutdown-exec.sh"
        done
        ;;
    "zookeeper")
        printf -- "${INFO}========== STOP ZOOKEEPER ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}--> Stop $host zookeeper.${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh stop"
        done
        ;;
    "kafka")
        printf -- "${INFO}========== STOP KAFKA ==========${END}\n"
        for host in ${kafka_nodes[@]}; do
            printf -- "${INFO}--> Stop $host kafka.${END}\n"
            ssh $HADOOP_USER@$host "$KAFKA_HOME/bin/kafka-server-stop.sh"
        done
        ;;
    "kettle")
        printf -- "${INFO}========== STOP KETTLE CLUSTERS ==========${END}\n"
        i=0
        for host in ${kettle_nodes[@]}; do
            printf -- "${INFO}--> Stop $host kettle.${END}\n"
            cmd="fuser -k 808$i/tcp"
            ssh $host $cmd
            let i+=1
        done
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot stop { hadoop | azkaban | zookeeper | kafka | kettle }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
status)
    case "$2" in
    "hadoop")
        printf -- "${INFO}========== HADOOP CLUSTERS STATUS ==========${END}\n"
        for host in ${workers[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host jps
        done
        printf -- "${INFO}----- hive services status -----${END}\n"
        check_process HiveMetastore 9083 >/dev/null && printf -- "${SUCCESS}HiveMetastore running normally.${END}\n" || printf -- "${WARN}HiveMetastore running abnormally.${END}\n"
        check_process HiveServer2 10000 >/dev/null && printf -- "${SUCCESS}HiveServer2 running normally${END}\n" || printf -- "${WARN}HiveServer2 running abnormally.${END}\n"
        ;;
    "azkaban")
        printf -- 'azkaban status\n'
        ;;
    "zookeeper")
        printf -- "${INFO}========== ZOOKEEPER STATUS ==========${END}\n"
        for host in ${zookeeper_nodes[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            ssh $HADOOP_USER@$host "$ZOOKEEPER_HOME/bin/zkServer.sh status"
        done
        ;;
    "kafka")
        printf -- 'kafka status\n'
        ;;
    "kettle")
        printf -- "${INFO}========== KETTLE STATUS ==========${END}\n"
        i=0
        for host in ${kettle_nodes[@]}; do
            printf -- "${INFO}----- $host -----${END}\n"
            cmd="netstat -nlp | grep 808$i"
            ssh $host $cmd
            let i+=1
        done
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot status { hadoop | azkaban | zookeeper | kafka | kettle }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
remove)
    case "$2" in
    "hadoop")
        if [ $HADOOP_HOME ] && [ -d $HADOOP_HOME ]; then
            # prompting for choice
            read -p "Do you want to remove all hadoop components. (y)Yes/(n)No:- " choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host hadoop -----${END}\n"
                    ssh $host "rm -rf /opt/marmot/*; cat /dev/null >/etc/profile.d/marmot_env.sh"
                done
                ;;
            [nN]*) printf -- "Remove cancelled" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The hadoop haven't been installed.${END}\n"
        fi
        ;;
    "azkaban")
        if [ $AZKABAN_HOME ] && [ -d $AZKABAN_HOME ]; then
            # prompting for choice
            read -p "Do you want to remove azkaban. (y)Yes/(n)No:- " choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host azkaban -----${END}\n"
                    ssh $host "rm -rf $AZKABAN_HOME"
                done
                ;;
            [nN]*) printf -- "Remove cancelled" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The azkaban haven't been installed.${END}\n"
        fi
        ;;
    "kafka")
        if [ $KAFKA_HOME ] && [ -d $AZKABAN_HOME ]; then
            # prompting for choice
            read -p "Do you want to remove kafka. (y)Yes/(n)No:- " choice
            case $choice in
            [yY]*)
                for host in ${workers[@]}; do
                    printf -- "${INFO}----- remove $host kafka -----${END}\n"
                    ssh $host "rm -rf $KAFKA_HOME"
                    ssh $host "rm -rf $ZOOKEEPER_HOME"
                done
                ;;
            [nN]*) printf -- "Remove cancelled" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kafka haven't been installed.${END}\n"
        fi
        ;;
    "kettle")
        if [ -d $PROJECT_DIR/data-integration ]; then
            # prompting for choice
            read -p "Do you want to remove kettle. (y)Yes/(n)No:- " choice
            case $choice in
            [yY]*)
                for host in ${kettle_nodes[@]}; do
                    printf -- "${INFO}----- remove $host kettle -----${END}\n"
                    ssh $host "rm -rf /opt/marmot/*; cat /dev/null >/etc/profile.d/marmot_env.sh"
                done
                ;;
            [nN]*) printf -- "Remove cancelled" ;;
            *) exit ;;
            esac
        else
            printf -- "${WARN}The kettle haven't been installed.${END}\n"
        fi
        ;;
    *)
        printf -- "${BOLD}USAGE: ${NORMAL}marmot remove { hadoop | azkaban | kafka | kettle }${END}\n"
        printf -- '\n'
        ;;
    esac
    ;;
*)
    echo "default (none of above)"
    ;;
esac
